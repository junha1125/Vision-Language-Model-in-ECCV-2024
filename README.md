# Vision Language Model in ECCV 2024

## Links

- [ECCV 2024 Calendar](https://eccv.ecva.net/virtual/2024/calendar)
- [ECCV 2024 Dates and Deadlines](https://eccv.ecva.net/Conferences/2024/Dates)
- [ECCV 2024 Schedule and Rooms](https://eccv.ecva.net/Conferences/2024/Schedule_and_rooms)
- [All ECCV Accepted Papers](https://docs.google.com/spreadsheets/d/1G8FQNlitoRr1oK2-LZEloeg0_VBP-E0J_WoSXqAhxNo/pubhtml#)



## Sun 29 Sep. Workshops

1. \[Tutorial\] Large Multimodal Foundation Models \[[Website](https://boyiliee.github.io/lmfm/)\]  
   Boyi Li, Sanjay Subramanian, Saining Xie, Trevor Darrell, Jitendra Malik  
    Time: 09:00 - 13:00  
    Location: Brown 3
2. \[Workshop\] Autonomous Vehicles meet Multimodal Foundation Models \[[Website](https://mllmav.github.io/)\]  
   Boris Ivanovic, Hongyang Li, Hang Zhao, Long Chen, Katerina Fragkiadaki  
    Time: 14:00 - 18:00  
    Location: Brown 2
3. \[Workshop\] Efficient Deep Learning for Foundation Models \[[Website](https://sites.google.com/view/efm24/home)\]  
   Hongxu (Danny) Yin, Sifei Liu, Ji Lin, Maying Shen, Jason Clemons, Xin Wang, Jose M. Alvarez, Pavlo Molchanov, Xueyan Zou, Xiaolong Wang, Song Han, Jan Kautz  
    Time: 14:00 - 18:00  
    Location: Brown 3
4. \[Workshop\] Enabling Complex Perception Through Vision and Language Foundational Models \[[Website](https://sites.google.com/view/omnilabel-workshop-eccv24/organizers)\]  
   Vijay Kumar B G, Yumin Suh, Samuel Schulter, Shiyu Zhao, Long Zhao, Dimitris N. Metaxas  
    Time: 14:00 - 18:00  
    Location: Amber 6




## Mon 30 Sep. Workshops & Tutorials

1. \[Workshop\] Out-of-Distribution Generalization in Computer Vision Foundation Models \[[Website](https://www.ood-cv.org/)\]  
   Sara Beery, Zsolt Kira, Jiahui Liu  
    Time: 09:00 - 13:00  
    Location: Brown 1
2. \[Workshop\] Multi-Agent Autonomous Systems Meet Foundation Models: Challenges and Futures \[[Website](https://coop-intelligence.github.io/)\]  
   Jiaqi Ma, Alois C. Knoll, Hang Qiu, Guillaume Sartoretti, Sheng Zhou, Amanda Prorok, Zsolt Kira, Manabu Tsukada, Marco Pavone, Jingwei Ji, Boris Ivanovic, Federico Tombari  
    Time: 14:00 - 18:00  
    Location: Amber 1
3. \[Workshop\] Foundation Models for 3D Humans \[[Website](https://human-foundation.github.io/workshop-eccv-2024/)\]  
   Michael J. Black, Jitendra Malik, Siyu Tang, Jingyi Yu, Angjoo Kanazawa  
    Time: 14:00 - 18:00  
    Location: Tower Lounge
4. \[Workshop\] Emergent Visual Abilities and Limits of Foundation Models \[[Website](https://sites.google.com/view/eval-fomo-24/home)\]  
   Antonio Torralba, Saining Xie, Heng Ji, Phillip Isola, Christian Rupprecht  
    Time: 14:00 - 18:00  
    Location: Amber 5
5. \[Workshop\] FOundation models Creators meet USers \[[Website](https://focus-workshop.github.io/)\]  
   Zsolt Kira, Ishan Misra, Hilde Kuehne  
    Time: 14:00 - 18:00  
    Location: Suite 2
6. \[Workshop\] Green Foundation Models \[[Website](https://green-fomo.github.io/ECCV2024/index.html)\]  
   Judy Hoffman, Ranjay Krishna, Elisabetta Farella, Jose M. Alvarez, Eric Schulz  
    Time: 14:00 - 18:00  
    Location: Suite 9





## Tue 1 Oct. Main Conference 1

### Poster 1 10:30-12:30

1. Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10330_ECCV_2024_paper.php)]  
    Location: # 142
2. Dolphins: Multimodal Language Model for Driving [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6203_ECCV_2024_paper.php)]  
    Location: # 341
3. FlexAttention for Efficient High-Resolution Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6203_ECCV_2024_paper.php)]  
    Location: # 133
4. Understanding Multi-compositional learning in Vision and Language models via Category Theory [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6472_ECCV_2024_paper.php)]  
    Location: # 96
5. BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual Cues [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10005_ECCV_2024_paper.php)]  
    Location: # 120
6. Controllable Contextualized Image Captioning: Directing the Visual Narrative through User-Defined Highlights [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6734_ECCV_2024_paper.php)]  
    Location: # 121





### Poster 2 16:30-18:30

1. Region-centric Image-Language Pretraining for Open-Vocabulary Detection [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8007_ECCV_2024_paper.php)]  
    Location: # 76 
2. Grid-Attention: Enhancing Computational Efficiency of Large Vision Models without Fine-Tuning [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6640_ECCV_2024_paper.php)]  
    Location: # 83
3. Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10172_ECCV_2024_paper.php)]  
    Location: # 103
4. Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7009_ECCV_2024_paper.php)]  
    Location: # 99
5. IVTP: Instruction-guided Visual Token Pruning for Large Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2577_ECCV_2024_paper.php)]  
    Location: # 101
6. SPHINX: A Mixer of Weights, Visual Embeddings and Image Scales for Multi-modal Large Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7894_ECCV_2024_paper.php)]  
    Location: # 94








## Tue 2 Oct. Main Conference 2

### Poster 1 10:30-12:30

1. SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5598_ECCV_2024_paper.php)]  
    Location: # 82
2. Self-Adapting Large Visual-Language Models to Edge Devices across Visual Modalities [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4094_ECCV_2024_paper.php)]  
    Location: # 90
3. Cascade Prompt Learning for Visual-Language Model Adaptation [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6701_ECCV_2024_paper.php)]  
    Location: # 83
4. CIC-BART-SSA: : Controllable Image Captioning with Structured Semantic Augmentation [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8373_ECCV_2024_paper.php)]  
    Location: # 98
5. Language-Image Pre-training with Long Captions [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2666_ECCV_2024_paper.php)]  
    Location: # 97
6. Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1023_ECCV_2024_paper.php)]  
    Location: # 109
7. The First to Know: How Token Distributions Reveal Hidden Knowledge in Large Vision-Language Models? [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6429_ECCV_2024_paper.php)]  
    Location: # 111
8. The Hard Positive Truth about Vision-Language Compositionality [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2149_ECCV_2024_paper.php)]  
    Location: # 100
9. ClearCLIP: Decomposing CLIP Representations for Dense Vision-Language Inference [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6346_ECCV_2024_paper.php)]  
    Location: # 86
10. Towards Open-Ended Visual Recognition with Large Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2233_ECCV_2024_paper.php)]  
    Location: # 87
11. SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language Guidance [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5598_ECCV_2024_paper.php)]  
     Location: # 82
12. Mind the Interference: Retaining Pre-trained Knowledge in Parameter Efficient Continual Learning of Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5329_ECCV_2024_paper.php)]  
     Location: # 72





### Poster 2 16:30-18:30

1. ControlCap: Controllable Region-level Captioning [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5494_ECCV_2024_paper.php)]
   Locatio: # 79
2. Adapt without Forgetting: Distill Proximity from Dual Teachers in Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7052_ECCV_2024_paper.php)]  
    Location: # 36
3. ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7792_ECCV_2024_paper.php)]  
    Location: # 84
4. MoAI: Mixture of All Intelligence for Large Language and Vision Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6579_ECCV_2024_paper.php)]  
    Location: # 85
5. Quantized Prompt for Efficient Generalization of Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2833_ECCV_2024_paper.php)]  
    Location: # 64
6. SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3178_ECCV_2024_paper.php)]  
    Location: # 77
7. Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3759_ECCV_2024_paper.php)]  
    Location: # 30






## Tue 3 Oct. Main Conference 3

### Poster 1 10:30-12:30

1. Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11636_ECCV_2024_paper.php)]  
    Location: # 158
2. Introducing Routing Functions to Vision-Language Parameter-Efficient Fine-Tuning with Low-Rank Bottlenecks [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/12192_ECCV_2024_paper.php)]  
    Location: # 154
3. MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7350_ECCV_2024_paper.php)]  
    Location: # 156
4. GalLop: Learning global and local prompts for vision-language models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7834_ECCV_2024_paper.php)]  
    Location: # 283
5. A Semantic Space is Worth 256 Language Descriptions: Make Stronger Segmentation Models with Descriptive Properties [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5527_ECCV_2024_paper.php)]  
    Location: # 54
6. Collaborative Vision-Text Representation Optimizing for Open-Vocabulary Segmentation [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4917_ECCV_2024_paper.php)]  
    Location: # 149



### Oral 2 13:30-15:30

1. LongVLM: Efficient Long Video Understanding via Large Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4936_ECCV_2024_paper.php)]  
    Location: 6B
2. GiT: Towards Generalist Vision Transformer through Universal Language Interface [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4158_ECCV_2024_paper.php)]  
    Location: 6C
3. Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3791_ECCV_2024_paper.php)]  
    Location: 6C 
4. Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6301_ECCV_2024_paper.php)]  
    Location: 6C
5. MMBENCH: Is Your Multi-Modal Model an All-around Player? [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/959_ECCV_2024_paper.php)]  
    Location: 6C
6. An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10478_ECCV_2024_paper.php)]  
    Location: 6C
7. uCAP: An Unsupervised Prompting Method for Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9508_ECCV_2024_paper.php)],   
    Location: 6C
8. BRAVE: Broadening the visual encoding of vision-language models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2433_ECCV_2024_paper.php)]  
    Location: 6C



### Poster 2 16:30-18:30

1. Conceptual Codebook Learning for Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9905_ECCV_2024_paper.php)]  
    Location: # 192
2. SILC: Improving Vision Language Pretraining with Self-Distillation [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3093_ECCV_2024_paper.php)]  
    Location: # 204
3. LongVLM: Efficient Long Video Understanding via Large Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4936_ECCV_2024_paper.php)]  
    Location: # 154
4. Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4914_ECCV_2024_paper.php)]  
    Location: # 199
5. Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6301_ECCV_2024_paper.php)]  
    Location: # 172
6. BRAVE: Broadening the visual encoding of vision-language models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/2433_ECCV_2024_paper.php)]  
    Location: # 190
7. uCAP: An Unsupervised Prompting Method for Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9508_ECCV_2024_paper.php)]  
    Location: # 202
8. An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/10478_ECCV_2024_paper.php)]  
    Location: # 169
9. Omniview-Tuning: Boosting Viewpoint Invariance of Vision-Language Pre-training Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3791_ECCV_2024_paper.php)]  
    Location: # 191
10. ViLA: Efficient Video-Language Alignment for Video Question Answering [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7919_ECCV_2024_paper.php)]  
    Location: # 274
11. ST-LLM: Large Language Models Are Effective Temporal Learners [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/7364_ECCV_2024_paper.php)]  
    Location: # 275
12. BLINK: Multimodal Large Language Models Can See but Not Perceive [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3356_ECCV_2024_paper.php)]  
    Location: # 189
13. Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8550_ECCV_2024_paper.php)]  
    Location: # 170
14. Improving Vision and Language Concepts Understanding with Multimodal Counterfactual Samples [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/8703_ECCV_2024_paper.php)]  
    Location: # 193
15. Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/610_ECCV_2024_paper.php)]  
    Location: # 175
16. CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/9810_ECCV_2024_paper.php)]  
    Location: # 187
17. Distractors-Immune Representation Learning with Cross-modal Contrastive Regularization for Change Captioning [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/5989_ECCV_2024_paper.php)]  
    Location: # 215





## Fri 4 Oct. Main Conference 4

### Oral 1 09:00-10:30

1. Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild
   [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3409_ECCV_2024_paper.php)]  
    Location: # Gold Room



### Poster 1 10:30-12:30

1. Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in the Wild
   [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/3409_ECCV_2024_paper.php)]  
    Location: # 145
2. LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models
   [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6290_ECCV_2024_paper.php)]  
    Location: # 20
3. SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant
   [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1393_ECCV_2024_paper.php)]  
    Location: # 7
4. Learning Chain of Counterfactual Thought for Bias-Robust Vision-Language Reasoning
   [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1309_ECCV_2024_paper.php)]  
    Location: # 22
5. BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models
   [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/1750_ECCV_2024_paper.php)]  
    Location: # 10
6. Attention Prompting on Image for Large Vision-Language Models
   [[pdf](https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/4374_ECCV_2024_paper.php)]  
    Location: # 12



